{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52235991-2c0f-455d-acaa-48d77bc7778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/yg334/anaconda3/envs/claude/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /common/home/yg334/anaconda3/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 111\n",
      "CUDA SETUP: Loading binary /common/home/yg334/anaconda3/envs/claude/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]=\"/common/users/yg334/LLAMA/huggingface/cache/\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from general_dataset import GeneralDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import openai\n",
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "from agi_utils import *\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from combine_model_seq import SeqCombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f833d5-40a7-4b56-8d76-df25583349b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:03<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "assign openagi data path \n",
    "\"\"\"\n",
    "# data_path = \"YOUR_DATA_PATH\"\n",
    "data_path = \"/common/users/yg334/openagi_data/\"\n",
    "\n",
    "task_discriptions = txt_loader(\"./task_description.txt\")\n",
    "# test_task_idx = [0,21,61,105,110,120,10,35,62,107,115]\n",
    "test_task_idx = [2,3,10,15,20,35,45,55,65,70,70,90,106,107]\n",
    "# test_task_idx = list(range(183))\n",
    "test_dataloaders = []\n",
    "for i in tqdm(test_task_idx):\n",
    "    dataset = GeneralDataset(i, data_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=5)\n",
    "    test_dataloaders.append(dataloader)\n",
    "    \n",
    "test_tasks = [task_discriptions[i].strip() for i in test_task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779c1ed7-7003-4f45-96f9-9fd0a5f544f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at nateraw/vit-base-beans and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# device_list = [\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\",\"cuda:7\",\"cpu\"]\n",
    "device_list = [\"cuda:0\",\"cuda:2\",\"cpu\"]\n",
    "eval_device = \"cuda:1\"\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load a pre-trained Vision Transformer model and its feature extractor\n",
    "vit_ckpt = \"nateraw/vit-base-beans\"\n",
    "vit = AutoModel.from_pretrained(vit_ckpt,cache_dir=\"/common/users/yg334/LLAMA/huggingface/cache/\")\n",
    "vit.eval()\n",
    "vit_extractor = AutoFeatureExtractor.from_pretrained(vit_ckpt,cache_dir=\"/common/users/yg334/LLAMA/huggingface/cache/\")\n",
    "\n",
    "f = transforms.ToPILImage()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\")\n",
    "\n",
    "seqCombination = SeqCombine(device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2283a04-1687-454f-b7d9-99a1ff6a798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "\n",
    "api_key = \"YOUR TOKEN\"\n",
    "anthropic = Anthropic(\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e0dd9c-c258-43d7-be01-64affb639f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR TOKEN\"\n",
    "\n",
    "def generate_module_list_with_gpt(generated_module_seq):\n",
    "    todo_prompt = \"You are a key phrase extractor who is able to extract potential module names from the given context. You have already known all the module names in the full module list. The full module list is: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Given the following context: '{}'. Please extract a module sequence from this context and remove module names which do not exist in the full module list from this sequence. Output the module sequence after filtering as the format of 'module: module1, module: module2, module: module3, etc...'. \"\n",
    "    prompt = todo_prompt.format(generated_module_seq)\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    content = completion.choices[0].message[\"content\"]\n",
    "    \n",
    "    # print(content)\n",
    "    \n",
    "    content = content.split(\"module: \")[1:]\n",
    "    \n",
    "    result = \"\"\n",
    "    for c in content:\n",
    "        result += c\n",
    "    \n",
    "    # result = result[:-1] if len(result) > 0 else result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# generated_module_list = generate_module_list_with_gpt(response[prompt_length:])\n",
    "# print(generated_module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678c0389-f753-4dfc-98ac-fb27b37d6392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given low-resolutioned blurry grayscale image, how to return the regular image step by step?\n",
      "Image Super Resolution, Image Denoising, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███                                        | 1/14 [01:49<23:39, 109.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given blurry grayscale image, how to return the regular image step by step?\n",
      "Image Deblurring, Image Denoising, Image Super Resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▎                                     | 2/14 [03:13<18:54, 94.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given low-resolutioned blurry image, how to return the regular image step by step?\n",
      "Image Super Resolution, Image Deblurring, Image Denoising\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████▏                                 | 3/14 [05:38<21:34, 117.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given low-resolutioned noisy blurry grayscale image, how to return the caption in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▌                               | 4/14 [05:56<13:01, 78.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Object Detection, Image Classification, Machine Translation, Text Summarization\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the object names in English step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████▋                            | 5/14 [06:08<08:08, 54.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Image Deblurring, Image Super Resolution, Object Detection, Image Classification\n",
      "Given blurry grayscale image, how to return the object names in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████▊                         | 6/14 [06:20<05:21, 40.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Deblurring, Object Detection, Image Denoising\n",
      "Given noisy grayscale image, how to return the caption in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 7/14 [06:32<03:36, 30.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Object Detection, Image Classification, Image Captioning\n",
      "Given low-resolutioned grayscale image, how to return the class label in English step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▏                  | 8/14 [06:42<02:25, 24.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Classification, Image Super Resolution\n",
      "Given low-resolutioned noisy blurry image, how to return the object names in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████▎               | 9/14 [06:55<01:42, 20.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Image Deblurring, Object Detection, Image Super Resolution, Image Classification, Machine Translation\n",
      "Given noisy blurry image, how to return the class label in German step by step?\n",
      "Image Denoising, Image Deblurring, Image Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████▋            | 10/14 [07:47<02:01, 30.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given noisy blurry image, how to return the class label in German step by step?\n",
      "Image Denoising, Image Super Resolution, Image Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████▊         | 11/14 [09:00<02:10, 43.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given low-resolutioned noisy image, how to return the caption in English step by step?\n",
      "Image Super Resolution, Image Denoising, Image Captioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████▊      | 12/14 [10:41<02:01, 60.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given English text, how to generate a image step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████▉   | 13/14 [10:52<00:45, 45.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Image Generation, Visual Question Answering, Image Super Resolution\n",
      "Given clozed English text, how to return the summarization in German step by step?\n",
      "Text Summarization, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [11:53<00:00, 50.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "clips = []\n",
    "berts = []\n",
    "similairies = []\n",
    "\n",
    "# eval_device = \"cuda:5\"\n",
    "\n",
    "task_len = len(test_tasks)\n",
    "\n",
    "for i, task_description in enumerate(tqdm(test_tasks)):\n",
    "    task_description = test_tasks[i]\n",
    "    print(task_description)\n",
    "    task_rewards = []\n",
    "    with torch.no_grad():\n",
    "        completion = anthropic.completions.create(\n",
    "                    model=\"claude-2\",\n",
    "                    max_tokens_to_sample=3000,\n",
    "                    prompt=f\"{HUMAN_PROMPT} {task_description} {AI_PROMPT}\",\n",
    "                )\n",
    "        module_list = generate_module_list_with_gpt(completion.completion)\n",
    "        print(module_list)\n",
    "    \n",
    "\n",
    "    if len(module_list) > 0 and whole_module_seq_filter(module_list, test_task_idx[i]):\n",
    "        seqCombination.construct_module_seq(module_list)\n",
    "\n",
    "        for idx, batch in enumerate(test_dataloaders[i]):\n",
    "            inputs = list(batch['input'][0])\n",
    "            predictions = seqCombination.run_module_seq(inputs)\n",
    "            \n",
    "            if 0 <= test_task_idx[i] <= 14:\n",
    "                outputs = list(batch['output'][0])\n",
    "                dist = image_similarity(predictions, outputs, vit, vit_extractor)\n",
    "                task_rewards.append(dist / 100)\n",
    "            elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "                outputs = list(batch['output'][0])\n",
    "                f1 = np.mean(txt_eval(predictions, outputs, bertscore, device=eval_device))\n",
    "                \n",
    "                task_rewards.append(f1)\n",
    "            else:\n",
    "                score = clip_score(predictions, inputs)\n",
    "                task_rewards.append(score.detach()/100)\n",
    "                \n",
    "        ave_task_reward = np.mean(task_rewards)    \n",
    "        seqCombination.close_module_seq()\n",
    "            \n",
    "    else:\n",
    "        ave_task_reward = 0\n",
    "        \n",
    "    print(ave_task_reward)\n",
    "        \n",
    "    if 0 <= test_task_idx[i] <= 14:\n",
    "        similairies.append(ave_task_reward)\n",
    "    elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "        berts.append(ave_task_reward)\n",
    "    else:\n",
    "        clips.append(ave_task_reward)\n",
    "\n",
    "    rewards.append(ave_task_reward)     \n",
    "    \n",
    "\n",
    "print(\"Finished testing!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0cf8df8-ba6c-4990-9443-774cee6ebbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.22933686961233613, 0.6858464152018229, 0.305061094938053)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clips), np.mean(berts), np.mean(similairies), (np.mean(clips)+np.mean(berts)+np.mean(similairies))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bdb31-47f4-41cf-ad6a-c16ad0fca8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claude",
   "language": "python",
   "name": "claude"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
