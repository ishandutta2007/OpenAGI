{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acb19ce-a28d-44cc-b982-417d3d0366c5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "os.chdir('../')\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]=\"/common/users/zl502/huggingface/cache/\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# from peft import PeftModel, PeftModelForCausalLM, prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "from general_dataset import GeneralDataset\n",
    "from agi_utils import *\n",
    "from tqdm import tqdm\n",
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "import random\n",
    "from evaluate import load\n",
    "from torchvision import transforms\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from combine_model_seq import SeqCombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a1e17d-2c29-4671-838a-d2f5185b1e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:05<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "assign openagi data path \n",
    "\"\"\"\n",
    "# data_path = \"YOUR_DATA_PATH\"\n",
    "data_path = \"/common/users/zl502/openagi_data/\"\n",
    "\n",
    "task_discriptions = txt_loader(\"./task_description.txt\")\n",
    "# test_task_idx = [0,21,61,105,110,120,10,35,62,107,115]\n",
    "test_task_idx = [2,3,10,15,20,35,45,55,65,70,70,90,106,107]\n",
    "# test_task_idx = list(range(183))\n",
    "test_dataloaders = []\n",
    "for i in tqdm(test_task_idx):\n",
    "    dataset = GeneralDataset(i, data_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=20)\n",
    "    test_dataloaders.append(dataloader)\n",
    "    \n",
    "test_tasks = [task_discriptions[i].strip() for i in test_task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f039c632-0136-4490-87ab-3fa1564e07ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648316034f8349efac765caf66bbf79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base_model = \"eachadea/vicuna-7b-1.1\"\n",
    "# base_model = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
    "# base_model = \"chainyo/alpaca-lora-7b\"\n",
    "base_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# base_model = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "# base_model = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "# base_model = \"chainyo/alpaca-lora-7b\"\n",
    "load_8bit = True\n",
    "\n",
    "hf_token = \"YOUR TOKEN\"\n",
    "\n",
    "max_memory_mapping = {\n",
    "    0: \"0GB\",\n",
    "    1: \"0GB\",\n",
    "    2: \"0GB\",\n",
    "    3: \"0GB\",\n",
    "    4: \"48GB\",\n",
    "    5: \"48GB\",\n",
    "    6: \"48GB\",\n",
    "    7: \"48GB\",\n",
    "}\n",
    "\n",
    "# max_memory_mapping = {\n",
    "#     0: \"0GB\",\n",
    "#     1: \"0GB\",\n",
    "#     2: \"24GB\",\n",
    "#     3: \"24GB\",\n",
    "# }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=\"/common/users/zl502/huggingface/cache\",\n",
    "    use_auth_token=hf_token\n",
    "    # padding_side='left'\n",
    ")\n",
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=\"/common/users/zl502/huggingface/cache\",\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory_mapping,\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d158a314-94e3-4493-b7d5-c2903a609403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[\"scores\"]\n",
    "import openai\n",
    "# prompt_length = len(input_text)\n",
    "openai.api_key = \"YOUR TOKEN\"\n",
    "\n",
    "def generate_module_list_with_gpt(generated_module_seq):\n",
    "    todo_prompt = \"You are a key phrase extractor who is able to extract potential module names from the given context. You have already known all the module names in the full module list. The full module list is: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Given the following context: '{}'. Please extract a module sequence from this context and remove module names which do not exist in the full module list from this sequence. Output the module sequence after filtering as the format of 'module: module1, module: module2, module: module3, etc...'. \"\n",
    "    prompt = todo_prompt.format(generated_module_seq)\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    content = completion.choices[0].message[\"content\"]\n",
    "    \n",
    "    # print(content)\n",
    "    \n",
    "    content = content.split(\"module: \")[1:]\n",
    "    \n",
    "    result = \"\"\n",
    "    for c in content:\n",
    "        result += c\n",
    "    \n",
    "    # result = result[:-1] if len(result) > 0 else result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# generated_module_list = generate_module_list_with_gpt(response[prompt_length:])\n",
    "# print(generated_module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08c5c70-002a-4231-b54f-62d1c06f6540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nateraw/vit-base-beans were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at nateraw/vit-base-beans and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f28de911cf4d53855b58ee745733bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading Evaluation Metrics\n",
    "\"\"\"\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "\n",
    "# Load a pre-trained Vision Transformer model and its feature extractor\n",
    "vit_ckpt = \"nateraw/vit-base-beans\"\n",
    "vit = AutoModel.from_pretrained(vit_ckpt)\n",
    "vit.eval()\n",
    "vit_extractor = AutoFeatureExtractor.from_pretrained(vit_ckpt)\n",
    "\n",
    "f = transforms.ToPILImage()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# device_list = [\"cuda:1\",\"cuda:2\",\"caugment_prompt:3\",\"cuda:4\",\"cuda:5\",\"cuda:7\",\"cpu\"]\n",
    "device_list = [\"cuda:0\", \"cpu\"]\n",
    "seqCombination = SeqCombine(device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c304bf7a-c38d-4c00-bbab-d0c378f0e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given low-resolutioned blurry grayscale image, how to return the regular image step by step?\n",
      "Given low-resolutioned blurry grayscale image, how to return the regular image step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to enhance and sharpen a low-resolutioned blurry grayscale image using various techniques. Please note that these are general steps and may require some adjustments based on the specific image you're working with.\n",
      "\n",
      "Step 1: Noise Reduction\n",
      "To start, we need to reduce the noise in the image. We can use a technique called \"Gaussian Blur\" to smooth out the image and remove any unwanted noise. This will help us improve the overall quality of the image before moving on to other steps.\n",
      "\n",
      "Step 2: Sharpening\n",
      "Once we have reduced the noise, we can move on to sharpening the image. There are several ways to do this, but one effective method is to use the \"Unsharp Mask\" filter. This filter will help bring out the details in the image and make it look more defined.\n",
      "\n",
      "Step 3: Contrast Enhancement\n",
      "Next, we want to enhance the contrast of the image so that the differences between light and dark areas become more pronounced. One way to do this is by adjusting the histogram of the image. By stretching the histogram towards the left side (darker tones) and the right side (brighter tones), we can create more contrast and make the image appear more vivid.\n",
      "\n",
      "Step 4: Edge Detection\n",
      "Now that we have enhanced the contrast, let's focus on detecting edges in the image. Edges give our image depth and definition, so it's important to emphasize them. We can use a technique called \"Canny Edge Detection\" to find the edges in the image and highlight them.\n",
      "\n",
      "Step 5: Image Restoration\n",
      "Finally, we want to restore the original details in the image that were lost due to the low resolution and blurriness. To do this, we can use a technique called \"Image Reconstruction.\" This involves filling in missing pixels and restoring the original shape of objects in the image.\n",
      "\n",
      "And there you have it – five steps to enhance and sharpen a low-resolutioned blurry grayscale image! With these techniques, you should be able to achieve high-quality results and breathe new life into your images.\n",
      "['Step 1: Noise Reduction', 'Step 2: Sharpening', 'Step 3: Contrast Enhancement', 'Step 4: Edge Detection', 'Step 5: Image Restoration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███                                        | 1/14 [02:09<27:59, 129.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list Image Denoising, Colorization, Object Detection\n",
      "0\n",
      "Given blurry grayscale image, how to return the regular image step by step?\n",
      "Given blurry grayscale image, how to return the regular image step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to enhance and sharpen a blurry grayscale image using Python:\n",
      "\n",
      "Step 1: Load the Image\n",
      "---------------\n",
      "\n",
      "First, load the blurry grayscale image into your Python script. You can use libraries like OpenCV or Pillow for this purpose. For example:\n",
      "```python\n",
      "import cv2\n",
      "\n",
      "blurry_image = cv2.imread('path/to/blurry/image', cv2.IMREAD_GRAYSCALE)\n",
      "```\n",
      "Step 2: Apply Bilateral Filtering\n",
      "---------------------------\n",
      "\n",
      "Bilateral filtering is an effective way to reduce noise in images while preserving edges. It works by weighting pixels based on their similarity in intensity and spatial distance. To apply bilateral filtering, you can use the following code:\n",
      "```python\n",
      "bilateral_filtered_image = cv2.bilateralFilter(blurry_image, 5, 90, 90)\n",
      "```\n",
      "Here, `cv2.bilateralFilter()` takes four arguments:\n",
      "\n",
      "* `image`: The input image.\n",
      "* `sigmaColor`: The standard deviation of the Gaussian filter applied to color channels (set to 5 here).\n",
      "* `sigmaSpace`: The standard deviation of the Gaussian filter applied to space coordinates (set to 90 here).\n",
      "* `iterations`: The number of iterations to perform (set to 90 here).\n",
      "\n",
      "Step 3: Apply Unsharp Masking\n",
      "--------------------------\n",
      "\n",
      "Unsharp masking is another technique used to enhance images. It involves subtracting a blurred version of the original image from the current image. This helps bring out details that are lost due to camera shake or other forms of motion blur. To apply unsharp masking, you can use the following code:\n",
      "```python\n",
      "unsharp_masked_image = cv2.addWeighted(blurry_image, 0.5, bilateral_filtered_image, 0, 0, 0)\n",
      "```\n",
      "Here, `cv2.addWeighted()` takes six arguments:\n",
      "\n",
      "* `image`: The first input image.\n",
      "* `weight`: A value between 0 and 1 that determines the contribution of the first input image to the output. In this case, we set it to 0.5 to give equal importance to both inputs.\n",
      "* `input2`: The second input image.\n",
      "* `gamma`: The amount of thresholding applied to the input images (set to 0 here).\n",
      "* `beta`: The amount of smoothing applied to the output (set to 0 here).\n",
      "* `iterations`: The number of iterations performed (set to 0 here).\n",
      "\n",
      "Step 4: Sharpen with Non-Maximum Suppression\n",
      "--------------------------------------\n",
      "\n",
      "Finally, to further improve the results, we can apply non-maximum suppression to remove any duplicate edges or artifacts introduced during the previous steps. We can do this using the following code:\n",
      "```python\n",
      "sharpened_image = cv2.nonmaxSuppression(unsharp_masked_image, 0.8, 0.5)\n",
      "```\n",
      "Here, `cv2.nonmaxSuppression()` takes three arguments:\n",
      "\n",
      "* `image`: The input image.\n",
      "* `threshold`: The minimum difference in intensity required to suppress a pixel (set to 0.8 here).\n",
      "* `type`: The type of suppression to apply (set to 0.5 here, which means only pixels with values above the threshold will be kept).\n",
      "\n",
      "And that's it! Your blurry grayscale image should now look much clearer and sharper after applying these techniques.\n",
      "['Step 1: Load the Image', 'Step 2: Apply Bilateral Filtering', 'Step 3: Apply Unsharp Masking', 'Step 4: Sharpen with Non-Maximum Suppression']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▏                                    | 2/14 [06:14<39:28, 197.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list Image Captioning, Image Denoising, Fill Mask\n",
      "0\n",
      "Given low-resolutioned blurry image, how to return the regular image step by step?\n",
      "Given low-resolutioned blurry image, how to return the regular image step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to enhance and sharpen a low-resolutioned blurry image using various techniques. Please note that the results may vary depending on the quality of the input image.\n",
      "\n",
      "Step 1: Noise Reduction\n",
      "The first step is to reduce the noise in the image. You can use a noise reduction tool or filter to remove any unwanted specks and grains. This will help improve the overall clarity of the image.\n",
      "\n",
      "Step 2: Sharpening\n",
      "Once you have reduced the noise, it's time to sharpen the image. There are several methods for doing this, including unsharp masking, kernel sharpening, and smart sharpening. Unsharp masking involves adjusting the contrast of the image to bring out more details, while kernel sharpening uses a mathematical formula to sharpen specific areas of the image. Smart sharpening is a more advanced technique that uses algorithms to identify edges and sharpen them without oversharpening other areas.\n",
      "\n",
      "Step 3: Deconvolution\n",
      "Deconvolution is a process that reverses the blurring effect caused by the camera sensor or lens. It works by estimating the original sharp image from the blurred one. There are several deconvolution algorithms available, such as Richardson-Lucy deconvolution and wavelet deconvolution. These algorithms can be applied separately or combined with other techniques for better results.\n",
      "\n",
      "Step 4: Edge Enhancement\n",
      "Edge enhancement techniques can further improve the sharpness of the image by highlighting boundaries between objects. Some common edge enhancement techniques include the Sobel operator, Prewitt operator, and Laplacian of Gaussian (LoG). These operators work by detecting changes in intensity along edges and emphasizing them.\n",
      "\n",
      "Step 5: Color Correction\n",
      "Finally, you can apply color correction techniques to improve the overall color balance and saturation of the image. This can involve adjusting brightness, contrast, and hue levels, as well as applying curves or tone mapping to optimize the colors.\n",
      "\n",
      "By following these steps, you should be able to significantly enhance and sharpen your low-resolutioned blurry images. Keep in mind that the order of the steps can vary depending on the specific image and desired outcome, so feel free to experiment and find what works best for you.\n",
      "['Step 1: Noise Reduction', 'Step 2: Sharpening', 'Step 3: Deconvolution', 'Step 4: Edge Enhancement', 'Step 5: Color Correction']\n",
      "Module list Image Denoising, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:09,  9.87s/it]\u001b[A\n",
      "2it [00:19,  9.78s/it]\u001b[A\n",
      "3it [00:28,  9.17s/it]\u001b[A\n",
      "4it [00:36,  9.02s/it]\u001b[A\n",
      "5it [00:46,  9.33s/it]\u001b[A\n",
      " 21%|█████████▏                                 | 3/14 [09:12<34:37, 188.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7521700439453125\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the caption in German step by step?\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the caption in German step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to recognize objects in a low-resolutioned noisy blurry grayscale image and generate a description of them in German.\n",
      "\n",
      "Step 1: Preprocessing\n",
      "First, we need to preprocess the input image to enhance its quality. We can use techniques such as filtering or thresholding to remove noise and improve contrast. For example, you could apply a median filter to reduce salt and pepper noise, or a Gaussian filter to smooth out the image. Additionally, you may want to normalize the pixel values to ensure that they fall within a reasonable range for processing.\n",
      "\n",
      "Step 2: Object Detection\n",
      "Next, we need to detect objects in the preprocessed image. One way to do this is through edge detection using algorithms like Canny edge detection or Sobel operators. These methods will help us identify lines and boundaries in the image that correspond to object edges. Once we have detected the edges, we can group them together to form recognizable objects.\n",
      "\n",
      "Step 3: Object Recognition\n",
      "Now that we have identified objects in the image, we need to determine what those objects are. This involves comparing features from our image with known features associated with specific objects. For instance, if we find an object with a round shape and two protrusions at the top, it might be recognized as a chair. To perform recognition, we can utilize machine learning models trained on large datasets of labeled images. Popular frameworks include TensorFlow, PyTorch, and OpenCV.\n",
      "\n",
      "Step 4: Description Generation\n",
      "Once we have successfully recognized objects in the image, we can generate a natural language description of them. In German, we would typically start with the definite article \"der\" followed by the noun describing the object (e.g., \"der Stuhl\" for chair). We could also add adjectives and other descriptive words to provide more context about the object's appearance, size, color, etc. Finally, we would end the sentence with a period.\n",
      "\n",
      "Here's an example of the complete process applied to your given task:\n",
      "\n",
      "Input Image: [Blurry Grayscale Image]\n",
      "\n",
      "Preprocessing: Apply median filter to remove noise; Normalize pixel values to [0, 1].\n",
      "\n",
      "Object Detection: Use Canny edge detection algorithm to identify edges in the preprocessed image. Group edges into recognizable objects.\n",
      "\n",
      "Object Recognition: Compare feature extractor from Step 3 with known features from a training dataset to classify each object.\n",
      "\n",
      "Description Generation: Generate a natural language description of each recognized object in German, starting with \"der\" and including relevant adjectives and information about size, color, etc. End sentences with periods.\n",
      "\n",
      "Output: A list of descriptions in German for all recognized objects in the input image.]\n",
      "['Step 1: Preprocessing', 'Step 2: Object Detection', 'Step 3: Object Recognition', 'Step 4: Description Generation']\n",
      "Module list Object Detection, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:22, 22.05s/it]\u001b[A\n",
      "2it [00:25, 11.16s/it]\u001b[A\n",
      "3it [00:29,  7.76s/it]\u001b[A\n",
      "4it [00:32,  6.13s/it]\u001b[A\n",
      "5it [00:36,  7.21s/it]\u001b[A\n",
      " 29%|████████████▎                              | 4/14 [12:33<32:13, 193.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45538244783878323\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the object names in English step by step?\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the object names in English step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to extract objects from a low-resolutioned noisy blurry grayscale image and recognize them in English. Please note that this is a complex task and may require some preprocessing and fine-tuning of the model.\n",
      "\n",
      "Step 1: Preprocess the Image\n",
      "Before we can start recognizing objects, we need to preprocess the input image to enhance its quality and remove any noise. You can use techniques such as filtering, normalization, or edge detection to improve the image's resolution and contrast.\n",
      "\n",
      "Step 2: Load a Model for Object Detection\n",
      "Next, you will need to load an object detection model that has been trained on a dataset similar to your target images. There are many open-source models available online, such as YOLO (You Only Look Once), SSD (Single Shot Detector), or Faster R-CNN (Region-based Convolutional Neural Network). Choose one that fits your needs and download it.\n",
      "\n",
      "Step 3: Ingest the Image Data\n",
      "Once you have loaded the model, ingest the low-resolutioned noisy blurry grayscale image into the model using a data pipeline or API call. The model should be able to process the image and generate bounding boxes around detected objects.\n",
      "\n",
      "Step 4: Extract Bounding Boxes and Object Names\n",
      "After the model has processed the image, retrieve the bounding boxes and corresponding object names from the output. Depending on the model architecture, these might be stored in different formats, such as JSON, XML, or a custom format specific to the model.\n",
      "\n",
      "Step 5: Post-Processing and Fine-Tuning\n",
      "Finally, perform post-processing steps to refine the recognized objects and their labels. This could involve spell correction, word segmentation, or adjusting the confidence scores based on the model's accuracy. If necessary, you can also fine-tune the model further by retraining it with additional labeled examples or adjusting hyperparameters.\n",
      "\n",
      "And that's it! With these five steps, you should now be able to extract objects from a low-resolutioned noisy blurry grayscale image and recognize them in English using an object detection model. Keep in mind that the actual implementation details may vary depending on the programming language and framework you choose, but the overall approach should remain the same.\"\n",
      "['Step 1: Preprocess the Image', 'Step 2: Load a Model for Object Detection', 'Step 3: Ingest the Image Data', 'Step 4: Extract Bounding Boxes and Object Names', 'Step 5: Post-Processing and Fine-Tuning']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████▎                           | 5/14 [14:38<25:17, 168.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list Text to Image Generation, Object Detection, Machine Translation\n",
      "0\n",
      "Given blurry grayscale image, how to return the object names in German step by step?\n",
      "Given blurry grayscale image, how to return the object names in German step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to return the object names in German given a blurry grayscale image:\n",
      "\n",
      "Step 1: Preprocessing (5 minutes)\n",
      "\n",
      "* Apply noise reduction techniques such as median filtering or bilateral filtering to reduce noise in the input image.\n",
      "* Use edge detection algorithms like Canny edge detection or Sobel operator to extract edges from the preprocessed image.\n",
      "* Threshold the edged image to obtain binary mask.\n",
      "\n",
      "Step 2: Object Detection (10 minutes)\n",
      "\n",
      "* Load a trained deep learning model for object detection (e.g., YOLOv3, SSD, Faster R-CNN).\n",
      "* Pass the preprocessed and thresholded image through the object detection model to get bounding boxes around detected objects.\n",
      "\n",
      "Step 3: Object Recognition (15 minutes)\n",
      "\n",
      "* For each bounding box, use a separate deep learning model for object recognition (e.g., VGGNet, ResNet, Inception Network).\n",
      "* Pass the region of interest inside the bounding box through the object recognition model to predict the class label of the object.\n",
      "\n",
      "Step 4: Postprocessing (5 minutes)\n",
      "\n",
      "* Use the predicted class labels to identify the objects in the image and their corresponding locations.\n",
      "* Generate a list of object names in German based on the identified objects and their locations.\n",
      "\n",
      "Here are some tips to improve your performance:\n",
      "\n",
      "* Use high-quality images with clear boundaries between objects.\n",
      "* Adjust the parameters of the deep learning models according to the specific task requirements.\n",
      "* Experiment with different combinations of preprocessing techniques and object detection/recognition models to achieve better results.\n",
      "\n",
      "I hope this helps you return the object names in German given a blurry grayscale image! If you have any further questions or need more guidance, feel free to ask.\n",
      "['Step 1: Preprocessing (5 minutes)', 'Step 2: Object Detection (10 minutes)', 'Step 3: Object Recognition (15 minutes)', 'Step 4: Postprocessing (5 minutes)']\n",
      "Module list Object Detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.48s/it]\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "2it [00:06,  3.37s/it]\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "3it [00:10,  3.38s/it]\u001b[A\n",
      "4it [00:12,  3.14s/it]\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "5it [00:16,  3.28s/it]\u001b[A\n",
      " 43%|██████████████████▍                        | 6/14 [16:24<19:39, 147.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5886177115142346\n",
      "Given noisy grayscale image, how to return the caption in German step by step?\n",
      "Given noisy grayscale image, how to return the caption in German step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to return the caption for a given noisy grayscale image in German.\n",
      "\n",
      "Step 1: Load the Noisy Grayscale Image\n",
      "-------------------------------------\n",
      "\n",
      "First, you need to load the noisy grayscale image into your program or model. You can use libraries like OpenCV or Pillow to load images in Python.\n",
      "```python\n",
      "import cv2\n",
      "img = cv2.imread('noisy_grayscale_image.png', cv2.IMREAD_GRAYSCALE)\n",
      "```\n",
      "Step 2: Preprocess the Image (Optional)\n",
      "------------------------------------------\n",
      "\n",
      "Next, you may want to preprocess the image to enhance its quality and remove any noise that might affect the OCR process. This can include filtering, thresholding, or other techniques to improve the contrast and clarity of the image.\n",
      "```scss\n",
      "# Apply Gaussian filter to reduce noise\n",
      "img = cv2.GaussianBlur(img, (5, 5), 0)\n",
      "\n",
      "# Threshold the image to convert it to binary\n",
      "_, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY)\n",
      "```\n",
      "Step 3: Use an OCR Model to Extract Text\n",
      "-------------------------------------------\n",
      "\n",
      "Now that you have preprocessed the image, you can use an OCR model to extract text from the image. There are many open-source OCR models available online, such as Tesseract OCR or Google Cloud Vision API.\n",
      "```python\n",
      "from google.cloud import vision\n",
      "\n",
      "client = vision.Client()\n",
      "response = client.text_annotation(path='/path/to/your/image.jpg')\n",
      "print(response['fullTextAnnotation'])\n",
      "```\n",
      "Step 4: Translate the Text to German\n",
      "--------------------------------------\n",
      "\n",
      "Once you have extracted the text from the image using an OCR model, you can translate it to German using a machine translation tool or service. There are many APIs available online, such as Google Translate API or Microsoft Cognitive Services.\n",
      "```python\n",
      "import requests\n",
      "\n",
      "translation = requests.get('https://api.microsoftcrm.com/v8.0/translate', params={\n",
      "    'languageFrom': 'en',\n",
      "    'languageTo': 'de',\n",
      "    'text': response['fullTextAnnotation']\n",
      "}).json()['translations'][0]['text']\n",
      "\n",
      "print(f\"The caption is {translation}.\")\n",
      "```\n",
      "And that's it! With these steps, you should be able to return the caption for a given noisy grayscale image in German using OCR and machine translation.\n",
      "['Step 1: Load the Noisy Grayscale Image', 'Step 2: Preprocess the Image (Optional)', 'Step 3: Use an OCR Model to Extract Text', 'Step 4: Translate the Text to German']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 7/14 [18:56<17:22, 149.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list Image Denoising, Text to Image Generation, Machine Translation\n",
      "0\n",
      "Given low-resolutioned grayscale image, how to return the class label in English step by step?\n",
      "Given low-resolutioned grayscale image, how to return the class label in English step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to return the class label of a low-resolutioned grayscale image using deep learning techniques. Please note that this is just one possible approach and there may be other methods available as well.\n",
      "\n",
      "Step 1: Preprocessing - Resize the input image to a fixed size (e.g., 256x256 pixels) while maintaining its aspect ratio. This will help improve computational efficiency and reduce memory usage during training. Next, normalize the pixel values between 0 and 1.\n",
      "\n",
      "Step 2: Build Model Architecture - Design an appropriate neural network architecture for your classification task. A simple yet effective option could be a Convolutional Neural Network (CNN) with the following layers:\n",
      "\t* Input layer (grayscale image)\n",
      "\t* Convolutional layer(s) with filters suitable for extracting features from small images (e.g., 32 filters, kernel sizes 3x3 or 5x5)\n",
      "\t* Activation function (e.g., ReLU)\n",
      "\t* Pooling layer (e.g., max pooling with stride 2)\n",
      "\t* Flatten layer\n",
      "\t* Output layer (softmax classifier)\n",
      "\n",
      "Step 3: Train Model - Split your dataset into training and validation sets (80%/20%). Use the training set to train the model until convergence, monitor loss and accuracy metrics regularly. Fine-tune hyperparameters as needed (learning rate, batch size). Save best checkpoint.\n",
      "\n",
      "Step 4: Evaluate Model - Test your trained model on the validation set. Calculate macro F1 score, precision, recall, and confusion matrix to evaluate performance. Adjust parameters if necessary.\n",
      "\n",
      "Step 5: Classify Image - Load pretrained model weights and feed the resized grayscale image through the model. Get output probabilities for each class. Find the highest probability class index and corresponding human-readable label. Return the label in English.\n",
      "\n",
      "That's it! With these steps, you should now have a basic understanding of how to use deep learning for image classification tasks with low-resolutioned grayscale inputs.\n",
      "['Step 1: Preprocessing - Resize the input image to a fixed size (e.g., 256x256 pixels) while maintaining its aspect ratio. This will help improve computational efficiency and reduce memory usage during training. Next, normalize the pixel values between 0 and 1.', 'Step 2: Build Model Architecture - Design an appropriate neural network architecture for your classification task. A simple yet effective option could be a Convolutional Neural Network (CNN) with the following layers:', 'Step 3: Train Model - Split your dataset into training and validation sets (80%/20%). Use the training set to train the model until convergence, monitor loss and accuracy metrics regularly. Fine-tune hyperparameters as needed (learning rate, batch size). Save best checkpoint.', 'Step 4: Evaluate Model - Test your trained model on the validation set. Calculate macro F1 score, precision, recall, and confusion matrix to evaluate performance. Adjust parameters if necessary.', 'Step 5: Classify Image - Load pretrained model weights and feed the resized grayscale image through the model. Get output probabilities for each class. Find the highest probability class index and corresponding human-readable label. Return the label in English.']\n",
      "Module list Image Super Resolution, Image Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:10, 10.77s/it]\u001b[A\n",
      "2it [00:21, 10.84s/it]\u001b[A\n",
      "3it [00:32, 10.93s/it]\u001b[A\n",
      "4it [00:43, 11.02s/it]\u001b[A\n",
      "5it [00:54, 10.94s/it]\u001b[A\n",
      " 57%|████████████████████████▌                  | 8/14 [21:41<15:24, 154.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8642942658066751\n",
      "Given low-resolutioned noisy blurry image, how to return the object names in German step by step?\n",
      "Given low-resolutioned noisy blurry image, how to return the object names in German step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to recognize objects in a low-resolutioned noisy blurry image and retrieve their names in German. Please note that this is a complex task and may require some preprocessing of the input images before proceeding.\n",
      "\n",
      "Step 1: Preprocess the Image\n",
      "Before we can start recognizing objects, we need to clean up the image and enhance its quality. Apply filters such as Gaussian Blur (to reduce noise), Median Filter (to remove artifacts), and Bilateral Filtering (to smooth out edges) to improve the overall appearance of the image. You can use libraries like OpenCV or TensorFlow for implementing these filters.\n",
      "\n",
      "Step 2: Object Detection\n",
      "Once you have preprocessed the image, you can use an object detection algorithm to identify objects within it. There are several popular algorithms available, including YOLO (You Only Look Once), SSD (Single Shot Detector), and Faster R-CNN (Region-based Convolutional Neural Network). These algorithms will give you bounding boxes around each detected object along with their class labels.\n",
      "\n",
      "Step 3a: Classify Objects into Categories\n",
      "Now that you have identified objects in the image, you need to categorize them based on their type. For example, if you detect a dog, cat, car, etc., you would assign those objects to their respective categories. To do this, you can train a machine learning model using labeled data sets. Common algorithms used for classification include SVM (Support Vector Machine), Random Forest, and K-Nearest Neighbors.\n",
      "\n",
      "Step 3b: Retrieve Names in German\n",
      "After classifying objects into categories, you can retrieve their names in German. For instance, if your object detector identifies a dog, you could look up its name in German from a dictionary or online resource. Some common resources include Google Translate, Dictionary.com, and WordReference.com.\n",
      "\n",
      "Step 4: Post-processing and Refining Results\n",
      "Finally, you may want to refine your results by removing any incorrect or redundant information. This could involve filtering out objects that were incorrectly detected or merging multiple instances of the same object category. Additionally, you might consider displaying the recognized objects with their corresponding German names to provide more context and meaning to the user.\n",
      "\n",
      "That concludes our step-by-step guide on recognizing objects in low-resolutioned noisy blurry images and retrieving their names in German. With the right tools and techniques, you should be able to achieve high accuracy and produce reliable results. Good luck!\n",
      "['Step 1: Preprocess the Image', 'Step 2: Object Detection', 'Step 3a: Classify Objects into Categories', 'Step 3b: Retrieve Names in German', 'Step 4: Post-processing and Refining Results']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████▋               | 9/14 [24:02<12:30, 150.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list Text to Image Generation, Object Detection, Image Classification, Machine Translation\n",
      "0\n",
      "Given noisy blurry image, how to return the class label in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████            | 10/14 [25:21<08:32, 128.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given noisy blurry image, how to return the class label in German step by step?\n",
      "### Assistant: I'd be happy to help! To return a class label from a noisy and blurry image in German, you can follow these steps:\n",
      "\n",
      "1. **Preprocessing**: Clean up the noise and sharpen the image using techniques like median filtering or Gaussian smoothing. You can use libraries like OpenCV for this purpose.\n",
      "2. **Object Detection**: Use an object detection algorithm such as YOLO (You Only Look Once), SSD (Single Shot Detector) or Faster R-CNN (Region-based Convolutional Neural Networks) to identify objects in the preprocessed image. These algorithms are widely used for detecting objects in images and can be implemented using popular deep learning frameworks like TensorFlow or PyTorch.\n",
      "3. **Classification**: After identifying objects in the image, apply classification techniques to determine their labels. For example, if you want to classify cars and buses, you could train a machine learning model on labeled data of both types and then use it to predict the class label of each detected object.\n",
      "4. **Postprocessing**: Finally, postprocess the predicted class labels to correct any errors that may have occurred during the prediction process. This might involve removing duplicate predictions, filling in missing values, or adjusting the confidence scores of the predictions.\n",
      "5. **Visualize results**: Visualize the results of your classification model to gain insights into its performance and accuracy. You can create visualizations like confusion matrices, precision-recall curves, or heatmaps to understand how well your model is performing.\n",
      "\n",
      "By following these steps, you should be able to return the class label of objects in a noisy and blurry image in German. Good luck with your project!\n",
      "[]\n",
      "Module list \n",
      "0\n",
      "Given noisy blurry image, how to return the class label in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████▊         | 11/14 [25:33<04:37, 92.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given noisy blurry image, how to return the class label in German step by step?\n",
      "### Assistant: Hello! I'd be happy to help you with that. To start, could you please provide me with the noisy blurry image and tell me what type of object or scene you would like to identify in the image? Additionally, do you have any specific requirements for the classification task, such as a certain level of accuracy or a particular programming language/framework you would like to use?\n",
      "[]\n",
      "Module list \n",
      "0\n",
      "Given low-resolutioned noisy image, how to return the caption in English step by step?\n",
      "Given low-resolutioned noisy image, how to return the caption in English step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to generate an accurate English caption for a given low-resolutioned noisy image using our AI model. Please note that this process may take some time and computational resources depending on the complexity of the image and the desired level of accuracy.\n",
      "\n",
      "Step 1: Preprocessing (5% of total processing time)\n",
      "\n",
      "* Resize the input image to a fixed size (e.g., 640x480 pixels) while maintaining its aspect ratio. This will help reduce computation and memory usage during the subsequent steps.\n",
      "* Apply bilateral filtering to smooth out noise in the image. This can be done using the BilateralFilter() function provided by OpenCV library. The parameters for the filter should be set as follows: sigma_color=3 and sigma_space=3.\n",
      "* Convert the preprocessed image into grayscale mode to simplify the feature extraction process.\n",
      "\n",
      "Step 2: Object Detection (15% of total processing time)\n",
      "\n",
      "* Use a pre-trained object detection model such as YOLOv3 or SSD to detect objects in the preprocessed image. You can use libraries like OpenCV or PyTorch to implement these models. Set the confidence threshold for object detection based on your specific requirements. For example, you might want to consider only objects with a confidence score above 0.7 or 0.8.\n",
      "* Extract bounding boxes around each detected object along with their corresponding class labels (e.g., person, car, tree, etc.). Store these outputs in a list called \"objects\" for later use.\n",
      "\n",
      "Step 3: Feature Extraction (20% of total processing time)\n",
      "\n",
      "* Utilize a pre-trained Convolutional Neural Network (CNN) model to extract relevant features from the preprocessed image. You can fine-tune the CNN model on a large dataset of images similar to yours to improve the accuracy of the captions. Some popular CNN architectures include VGGNet, InceptionV3, and ResNet.\n",
      "* Once the features are extracted, flatten them into a one-dimensional array to prepare for the next step.\n",
      "\n",
      "Step 4: Sentence Generation (60% of total processing time)\n",
      "\n",
      "* Now it's time to generate the actual sentence describing the content of the image. To do so, we'll employ a Recurrent Neural Network (RNN) language model that takes the extracted features as input and generates a natural language description as output. You can choose from various RNN architectures such as LSTM or GRU, both of which have been widely used in Natural Language Processing (NLP) tasks.\n",
      "* During training, provide the ground truth sentences for each image in your training dataset. The RNN model will learn to predict the correct sentence based on the input features.\n",
      "\n",
      "Step 5: Postprocessing (10% of total processing time)\n",
      "\n",
      "* Finally, we'll perform some postprocessing techniques to refine the generated sentences and make them more coherent and human-like. We can apply techniques such as spell correction, grammar correction, word substitution, and fluency evaluation. These processes can be performed using NLTK or spaCy libraries.\n",
      "\n",
      "And that's it! With these five steps, you now have a comprehensive framework for generating accurate English captions for low-resolutioned noisy images using deep learning techniques.\n",
      "['Step 1: Preprocessing (5% of total processing time)', 'Step 2: Object Detection (15% of total processing time)', 'Step 3: Feature Extraction (20% of total processing time)', 'Step 4: Sentence Generation (60% of total processing time)', 'Step 5: Postprocessing (10% of total processing time)']\n",
      "Module list Object Detection, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:04,  4.13s/it]\u001b[A\n",
      "2it [00:07,  3.56s/it]\u001b[A\n",
      "3it [00:11,  3.77s/it]\u001b[A\n",
      "4it [00:14,  3.46s/it]\u001b[A\n",
      "5it [00:18,  3.65s/it]\u001b[A\n",
      " 86%|████████████████████████████████████      | 12/14 [29:21<04:27, 133.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44406582504510883\n",
      "Given English text, how to generate a image step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████   | 13/14 [31:04<02:04, 124.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given English text, how to generate a image step by step?\n",
      "### Assistant: Sure! Here's an outline of the steps involved in generating an image from given English text using Deep Learning techniques:\n",
      "\n",
      "1. Text Preprocessing: The input text is preprocessed to convert it into a numerical representation that can be fed into a neural network. This includes tokenization, stemming or lemmatizing, removing stop words and punctuation, and converting all texts to lowercase.\n",
      "2. Tokenizer: A tokenizer is used to break down the text into individual tokens, such as words or phrases. These tokens are then fed into the next step.\n",
      "3. Embeddings: Each token is embedded into a vector space using a learned embedding layer. This allows the model to understand the meaning of each word in relation to other words.\n",
      "4. Encoder: An encoder module takes the embedded tokens and generates a continuous representation of the input text. This could be done using Recurrent Neural Network (RNN) like LSTM or GRU or Transformer architecture.\n",
      "5. Decoder: A decoder module uses this continuous representation to generate the output image. In most cases, the decoder will take the form of another RNN, which predicts the pixels of the output image one at a time.\n",
      "6. Attention Mechanism: To help the model focus on important parts of the input text when generating the output image, an attention mechanism is typically employed. This involves computing a weighted sum of the input tokens based on their relevance to the current output pixel being generated.\n",
      "7. Pixels Generation: Finally, the decoder generates the output image, one pixel at a time, using the computed weights and the previous pixels information.\n",
      "8. Post-Processing: After generation of the image, there may be some post-processing steps like resizing, cropping, normalization etc. to make the final output more visually appealing and suitable for the task at hand.\n",
      "\n",
      "Keep in mind that these are just general steps, and different models may have slightly different architectures or variations on these steps. But overall, this gives you a good idea of how deep learning models work to generate images from text inputs!\n",
      "[]\n",
      "Module list \n",
      "0\n",
      "Given clozed English text, how to return the summarization in German step by step?\n",
      "Given clozed English text, how to return the summarization in German step by step?\n",
      "### Assistant: Sure! Here's a step-by-step guide on how to return the summarization of a given closed English text in German.\n",
      "\n",
      "Step 1: Identify the Key Points\n",
      "The first step is to identify the key points or main ideas of the given closed English text. This can be done by reading the text carefully and highlighting or underlining important information. You can also use techniques such as skimming or scanning to quickly identify the most relevant parts of the text.\n",
      "\n",
      "Step 2: Translate the Text into German\n",
      "Once you have identified the key points, translate the entire text into German using a translation tool like Google Translate or DeepL. Make sure to choose the \"Automatic\" or \"Neural\" translation option for more accurate results.\n",
      "\n",
      "Step 3: Summarize the Text in German\n",
      "After translating the text into German, summarize it in your own words. Focus on capturing the main ideas and key points from the original text. Use simple language and avoid including unnecessary details.\n",
      "\n",
      "Step 4: Edit and Refine Your Summary\n",
      "Review your summary and edit it as needed to ensure that it accurately reflects the content of the original text. You may need to make adjustments based on grammar, syntax, or word choice to ensure clarity and accuracy.\n",
      "\n",
      "And there you have it - four easy steps to returning the summarization of a given closed English text in German! By following these steps, you should be able to create an accurate and concise summary of any given text in just a few minutes.\n",
      "['Step 1: Identify the Key Points', 'Step 2: Translate the Text into German', 'Step 3: Summarize the Text in German', 'Step 4: Edit and Refine Your Summary']\n",
      "Module list Machine Translation, Text Summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:08,  8.98s/it]\u001b[A\n",
      "2it [00:15,  7.30s/it]\u001b[A\n",
      "3it [00:21,  6.81s/it]\u001b[A\n",
      "4it [00:29,  7.15s/it]\u001b[A\n",
      "5it [00:40,  8.07s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████| 14/14 [32:57<00:00, 141.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1968381442129612\n",
      "Finished testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\")\n",
    "\n",
    "\n",
    "rewards = []\n",
    "clips = []\n",
    "berts = []\n",
    "similairies = []\n",
    "\n",
    "module_length = 10\n",
    "num_beams = 1\n",
    "num_return_sequences = 1\n",
    "\n",
    "eval_device = \"cuda:1\"\n",
    "\n",
    "for i, task_description in enumerate(tqdm(test_tasks)):\n",
    "    print(task_description)\n",
    "    task_rewards = []\n",
    "    with torch.no_grad():\n",
    "        input_s = [\"### Human: \"+ task_description +\"\\n### Assistant:\"]\n",
    "        input_ids = tokenizer.batch_encode_plus(\n",
    "            input_s, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].to(eval_device)\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=2048, \n",
    "            return_dict_in_generate=True, \n",
    "            output_scores=True, \n",
    "            output_hidden_states=True,\n",
    "            repetition_penalty=1.25\n",
    "        )\n",
    "    generated_seq = tokenizer.decode(\n",
    "        output[\"sequences\"][0], skip_special_tokens=True, temperature=0, top_p=0.8, repetition_penalty=1.25\n",
    "    )\n",
    "\n",
    "    # print(generated_seq)\n",
    "\n",
    "    # print(generated_seq.split(\"### Human: \"))\n",
    "    \n",
    "    llama_solution = generated_seq.split(\"### Human: \")[1]\n",
    "    print(llama_solution)\n",
    "    llama_solution = llama_solution.split(\"\\n\")\n",
    "\n",
    "    llama_steps = []\n",
    "    for l,j in enumerate(llama_solution):\n",
    "        if j[0:4] == \"Step\":\n",
    "            llama_steps.append(llama_solution[l])\n",
    "    print(llama_steps)\n",
    "                \n",
    "    module_list = match_module_seq(llama_steps, sentence_model)\n",
    "    print(\"Module list: \", module_list)\n",
    "    \n",
    "    # vicuna_steps = generate_module_list_with_gpt(generated_seq).split(\",\")\n",
    "    # module_list = match_module_seq(vicuna_steps, sentence_model)\n",
    "    # print(module_list)\n",
    "\n",
    "    # if len(module_list) >= 1 and module_seq_filter(module_list, test_task_idx[i]):\n",
    "    if len(module_list) >= 1 and whole_module_seq_filter(module_list, test_task_idx[i]):\n",
    "        seqCombination.construct_module_seq(module_list)\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(test_dataloaders[i])):\n",
    "            inputs = list(batch['input'][0])\n",
    "            # print(\"Inputs: \", inputs)\n",
    "            try:\n",
    "                predictions = seqCombination.run_module_seq(inputs)\n",
    "            except:\n",
    "                ave_task_reward = 0\n",
    "                break\n",
    "\n",
    "            if 0 <= test_task_idx[i] <= 14:\n",
    "                outputs = list(batch['output'][0])\n",
    "                dist = image_similarity(predictions, outputs, vit, vit_extractor)\n",
    "                task_rewards.append(dist / 100)\n",
    "            elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "                outputs = list(batch['output'][0])\n",
    "                f1 = np.mean(txt_eval(predictions, outputs, bertscore, device=eval_device))\n",
    "                \n",
    "                task_rewards.append(f1)\n",
    "            else:\n",
    "                score = clip_score(predictions, inputs)\n",
    "                task_rewards.append(score.detach() / 100)\n",
    "                \n",
    "        ave_task_reward = np.mean(task_rewards)    \n",
    "        seqCombination.close_module_seq()\n",
    "            \n",
    "    else:\n",
    "        ave_task_reward = 0\n",
    "\n",
    "    print(ave_task_reward)\n",
    "        \n",
    "    if 0 <= test_task_idx[i] <= 14:\n",
    "        similairies.append(ave_task_reward)\n",
    "    elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "        berts.append(ave_task_reward)\n",
    "    else:\n",
    "        clips.append(ave_task_reward)\n",
    "\n",
    "    rewards.append(ave_task_reward)     \n",
    "    \n",
    "\n",
    "print(\"Finished testing!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9080eca7-0216-41b7-a09f-c3260de693f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.2549198394417763, 0.2507233479817708, 0.1685477291411824]\n"
     ]
    }
   ],
   "source": [
    "avg_clips = np.mean(clips)\n",
    "avg_berts = np.mean(berts)\n",
    "avg_similarities = np.mean(similairies)\n",
    "avg_rewards = (avg_clips + avg_berts + avg_similarities) / 3\n",
    "print([avg_clips, avg_berts, avg_similarities, avg_rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36123420-d640-4233-ab61-029d9827b86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "agi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
