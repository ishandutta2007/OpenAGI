{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52235991-2c0f-455d-acaa-48d77bc7778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/cbim/vast/zl502/anaconda3/envs/peft_agi/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /research/cbim/vast/zl502/anaconda3/envs/peft_agi/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "os.chdir('../')\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]=\"/common/users/zl502/huggingface/cache/\"\n",
    "from general_dataset import GeneralDataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from agi_utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f833d5-40a7-4b56-8d76-df25583349b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/common/users/zl502/openagi_data/\"\n",
    "task_discriptions = txt_loader(\"./task_description.txt\")\n",
    "# task_idx = [0,21,61,105,110,120,10,35,62,107,115]\n",
    "test_task_idx = [2,3,10,15,20,35,45,55,65,70,70,90,106,107]\n",
    "test_dataloaders = []\n",
    "for i in test_task_idx:\n",
    "    dataset = GeneralDataset(i, data_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=20)\n",
    "    test_dataloaders.append(dataloader)\n",
    "    \n",
    "test_tasks = [task_discriptions[i].strip() for i in test_task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2748b625-77d6-4343-955c-2e4b453c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_solution = []\n",
    "with open('./train_model_sequence.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:50]:\n",
    "        train_solution.append(line)\n",
    "f.close()\n",
    "\n",
    "train_tasks = []\n",
    "with open('./train_task_description.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:50]:\n",
    "        train_tasks.append(line)\n",
    "f.close()\n",
    "\n",
    "context = \"\"\n",
    "for i in range(len(train_tasks)):\n",
    "    if i >= 15:\n",
    "        break\n",
    "    steps = \"\"\n",
    "    for index,j in enumerate(train_solution[i].split(',')):\n",
    "        steps += \"Step \" + str(index+1) + \":\" + j.strip(\"\\n\") + \", \\n\"\n",
    "    cur = \"Problem: \" + train_tasks[i] +  \"Solution:\\n\" + steps\n",
    "    context += cur\n",
    "    \n",
    "# print(context + \"###Human: \" + test_tasks[0]+\"\\###Assistant: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb7c5d8-5040-42bb-8d2d-f42df0fa55b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bdf785f3d34bf882a8e416ab9fee66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "base_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "cache_dir = \"/common/users/zl502/huggingface/cache\"\n",
    "# base_model = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "# base_model = \"chainyo/alpaca-lora-7b\"\n",
    "load_8bit = True\n",
    "\n",
    "hf_token = \"YOUR TOKEN\"\n",
    "\n",
    "# max_memory_mapping = {0: \"18GB\", 1: \"18GB\", 2: \"18GB\", 3: \"18GB\", 4: \"18GB\", 5: \"18GB\", 6: \"18GB\", 7: \"18GB\",}\n",
    "max_memory_mapping = {\n",
    "    0: \"48GB\", \n",
    "    1: \"48GB\", \n",
    "    2: \"48GB\", \n",
    "    3: \"48GB\", \n",
    "    4: \"0GB\", \n",
    "    5: \"0GB\", \n",
    "    6: \"0GB\", \n",
    "    7: \"0GB\",\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model, \n",
    "    cache_dir=cache_dir, \n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    cache_dir=cache_dir,\n",
    "    # load_in_8bit=True,\\\n",
    "    device_map='auto',\n",
    "    max_memory = max_memory_mapping,\n",
    "    use_auth_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17876034-45e8-4e38-bb16-6f464e16db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer= torch.nn.DataParallel(tokenizer,device_ids = [0,1,2,3])\n",
    "# model= torch.nn.DataParallel(model,device_ids = [0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e22427-e6c2-43a2-aabc-9928dcda564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nateraw/vit-base-beans were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at nateraw/vit-base-beans and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdc0b63603c4f9291f963f134b65640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 20 files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "from agi_utils import *\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from combine_module_seq import SeqCombine\n",
    "\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load a pre-trained Vision Transformer model and its feature extractor\n",
    "vit_ckpt = \"nateraw/vit-base-beans\"\n",
    "vit = AutoModel.from_pretrained(vit_ckpt)\n",
    "vit.eval()\n",
    "vit_extractor = AutoFeatureExtractor.from_pretrained(vit_ckpt)\n",
    "\n",
    "f = transforms.ToPILImage()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# device_list = [\"cuda:1\",\"cuda:2\",\"caugment_prompt:3\",\"cuda:4\",\"cuda:5\",\"cuda:7\",\"cpu\"]\n",
    "device_list = [\"cuda:5\", \"cpu\"]\n",
    "seqCombination = SeqCombine(device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b922a53d-838d-4605-bf69-ed0e32467534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# prompt_length = len(input_text)\n",
    "openai.api_key = \"YOUR TOKEN\"\n",
    "\n",
    "def generate_module_list_with_gpt(generated_module_seq):\n",
    "    todo_prompt = \"You are a key phrase extractor who is able to extract potential module names from the given context. You have already known all the module names in the full module list. The full module list is: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Given the following context: '{}'. Please extract a module sequence from this context and remove module names which do not exist in the full module list from this sequence. Output the module sequence after filtering as the format of 'module: module1, module: module2, module: module3, etc...'. \"\n",
    "    prompt = todo_prompt.format(generated_module_seq)\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    content = completion.choices[0].message[\"content\"]\n",
    "    \n",
    "    # print(content)\n",
    "    \n",
    "    content = content.split(\"module: \")[1:]\n",
    "    \n",
    "    result = \"\"\n",
    "    for c in content:\n",
    "        result += c\n",
    "    \n",
    "    # result = result[:-1] if len(result) > 0 else result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678c0389-f753-4dfc-98ac-fb27b37d6392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n",
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:46, 46.40s/it]\u001b[A\n",
      "2it [01:28, 44.16s/it]\u001b[A\n",
      "3it [02:11, 43.38s/it]\u001b[A\n",
      "4it [02:55, 43.45s/it]\u001b[A\n",
      "5it [03:38, 43.65s/it]\u001b[A\n",
      "  7%|███                                        | 1/14 [04:17<55:43, 257.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7065360565185547\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n",
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:42, 42.26s/it]\u001b[A\n",
      "2it [01:23, 41.78s/it]\u001b[A\n",
      "3it [02:06, 42.09s/it]\u001b[A\n",
      "4it [02:48, 42.26s/it]\u001b[A\n",
      "5it [03:32, 42.53s/it]\u001b[A\n",
      " 14%|██████▏                                    | 2/14 [08:51<53:26, 267.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7033295440673828\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n",
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:42, 42.97s/it]\u001b[A\n",
      "2it [01:26, 43.55s/it]\u001b[A\n",
      "3it [02:10, 43.71s/it]\u001b[A\n",
      "4it [02:53, 43.15s/it]\u001b[A\n",
      "5it [03:35, 43.15s/it]\u001b[A\n",
      " 21%|█████████▏                                 | 3/14 [13:03<47:44, 260.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6973706054687498\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▎                              | 4/14 [15:57<37:42, 226.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████▎                           | 5/14 [16:35<23:45, 158.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████▍                        | 6/14 [17:12<15:35, 116.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 7/14 [17:40<10:14, 87.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▏                  | 8/14 [17:52<06:23, 63.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████▎               | 9/14 [18:41<04:55, 59.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████            | 10/14 [23:10<08:15, 123.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████▊         | 11/14 [23:53<04:57, 99.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████▊      | 12/14 [25:31<03:17, 98.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████▉   | 13/14 [25:41<01:11, 71.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "['Step 1:Image Super Resolution, ', 'Step 2: Image Denoising, ', 'Step 3: Image Deblurring, ', 'Step 4: Colorization, ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 14/14 [25:59<00:00, 111.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module list:  Image Super Resolution, Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "Finished testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\")\n",
    "\n",
    "\n",
    "rewards = []\n",
    "clips = []\n",
    "berts = []\n",
    "similarities = []\n",
    "\n",
    "module_length = 10\n",
    "num_beams = 1\n",
    "num_return_sequences = 1\n",
    "\n",
    "\n",
    "eval_device = torch.device(\"cuda:4\")\n",
    "\n",
    "for i, task_description in enumerate(tqdm(test_tasks)):\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "    # print(task_description)\n",
    "    task_rewards = []\n",
    "    with torch.no_grad():\n",
    "        input_s = [context + \"Problem: \" + task_description + \"Solution:\\n\"]\n",
    "        # print(input_s)\n",
    "        input_ids = tokenizer.batch_encode_plus(\n",
    "            input_s, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].to(eval_device)\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=2048, return_dict_in_generate=True, output_scores=True, output_hidden_states=True,repetition_penalty=1.25\n",
    "        )\n",
    "    generated_seq = tokenizer.decode(\n",
    "        output[\"sequences\"][0], skip_special_tokens=True, temperature=0, top_p=0.8, repetition_penalty=1.25\n",
    "    )\n",
    "\n",
    "    # print(generated_seq)\n",
    "    \n",
    "    llama_solution = generated_seq.split(\"Problem: \")[1]\n",
    "    # print(llama_solution)\n",
    "    llama_solution = llama_solution.split(\"\\n\")\n",
    "\n",
    "    llama_steps = []\n",
    "    for l,j in enumerate(llama_solution):\n",
    "        if j[0:4] == \"Step\":\n",
    "            llama_steps.append(llama_solution[l])\n",
    "\n",
    "    print(llama_steps)\n",
    "    # llama_steps = generate_module_list_with_gpt(generated_seq[len(input_s[0]):]).split(\",\")\n",
    "    module_list = match_module_seq(llama_steps, sentence_model)\n",
    "    print(\"Module list: \", module_list)\n",
    "\n",
    "    if len(module_list) >= 1 and whole_module_seq_filter(module_list, test_task_idx[i]):\n",
    "        seqCombination.construct_module_seq(module_list)\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(test_dataloaders[i])):\n",
    "            inputs = list(batch['input'][0])\n",
    "            # print(\"Inputs: \", inputs)\n",
    "            try:\n",
    "                predictions = seqCombination.run_module_seq(inputs)\n",
    "            except:\n",
    "                ave_task_reward = 0\n",
    "                break\n",
    "\n",
    "            if 0 <= test_task_idx[i] <= 14:\n",
    "                outputs = list(batch['output'][0])\n",
    "                dist = image_similarity(predictions, outputs, vit, vit_extractor)\n",
    "                task_rewards.append(dist / 100)\n",
    "            elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "                outputs = list(batch['output'][0])\n",
    "                f1 = np.mean(txt_eval(predictions, outputs, bertscore, device=eval_device))\n",
    "                \n",
    "                task_rewards.append(f1)\n",
    "            else:\n",
    "                score = clip_score(predictions, inputs)\n",
    "                task_rewards.append(score.detach() / 100)\n",
    "                \n",
    "        ave_task_reward = np.mean(task_rewards)    \n",
    "        seqCombination.close_module_seq()\n",
    "            \n",
    "    else:\n",
    "        ave_task_reward = 0\n",
    "\n",
    "    print(ave_task_reward)\n",
    "        \n",
    "    if 0 <= test_task_idx[i] <= 14:\n",
    "        similarities.append(ave_task_reward)\n",
    "    elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "        berts.append(ave_task_reward)\n",
    "    else:\n",
    "        clips.append(ave_task_reward)\n",
    "\n",
    "    rewards.append(ave_task_reward)     \n",
    "    \n",
    "\n",
    "print(\"Finished testing!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b30ce0a-4d33-4810-8acb-f73ccd4b6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(input_s[0])\n",
    "# print(generated_seq[len(input_s[0]):])\n",
    "# vicuna_steps = generate_module_list_with_gpt(generated_seq[len(input_s[0]):]).split(\",\")\n",
    "# module_list = match_module_seq(vicuna_steps, sentence_model)\n",
    "# print(module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186332c4-112e-4bfb-a876-f81a95734360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.7024120686848958, 0.2341373562282986]\n"
     ]
    }
   ],
   "source": [
    "avg_clips = np.mean(clips)\n",
    "avg_berts = np.mean(berts)\n",
    "avg_similarities = np.mean(similarities)\n",
    "avg_rewards = (avg_clips + avg_berts + avg_similarities) / 3\n",
    "print([avg_clips, avg_berts, avg_similarities, avg_rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1969a-2876-4832-a4e2-f8769d5dd1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "agi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
